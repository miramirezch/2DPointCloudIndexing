{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2D PointCloud Indexing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IGI Spark\n",
    "Inverted Grid Index for PointClouds with Apache Spark 2.0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# From Apache Spark - Pyspark\n",
    "import findspark\n",
    "findspark.init()\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkContext, SparkConf, StorageLevel\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "# Python\n",
    "import time\n",
    "from statistics import mode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apache Spark Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Set configuration according with your Server/Cluster\n",
    "sc = (SparkSession.builder.appName(\"IGI Spark\")      \n",
    "      .master(\"Set master URL\")\n",
    "      .config(\"spark.executor.cores\", \"5\")\n",
    "      .config(\"spark.executor.instances\", \"12\")\n",
    "      .config(\"spark.executor.memory\", \"20g\")      \n",
    "      .config(\"spark.driver.memory\", \"8g\")\n",
    "      .config(\"spark.driver.cores\", \"1\")   \n",
    "      .config(\"spark.python.worker.memory\", \"70g\")                \n",
    "      .enableHiveSupport()\n",
    "      .getOrCreate())\n",
    "\n",
    "# (Optional- For large files) - .config(\"spark.sql.shuffle.partitions\", \"10000\")  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IGI Parameters\n",
    "Set inverted grid index parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cmax = 10000\n",
    "delta = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading PointClouds from CSV File for Indexing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Schema from CSV:\n",
    "# Example.csv\n",
    "# 1st Row: ID, X, Y\n",
    "# 2nd Row: 0, 2.4, 1.2\n",
    "# 3rd Row: 0, 4.2, 11.5\n",
    "# 4th Row: 1, 4.3, 12.5\n",
    "# 5th Row: 1, 64.2, 12.6\n",
    "# ...\n",
    "# ith Row: 501, 52.5,12.5\n",
    "# (i+1)th Row: 501, 23.5,213.5\n",
    "\n",
    "df = sc.read.format('csv').options(header='true', inferschema='true').csv('Path from the CSV file')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IGI Spark - Construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# For every point calculate corresponding cell\n",
    "datos = df.select(df['I'].cast(IntegerType()), floor(df['X']/delta).alias(\"X\"), floor(df['Y']/delta).alias(\"Y\"))\n",
    "idx = datos.select((datos['X']+ (cmax/delta)*datos['Y']).alias(\"Celda\"),datos['I'].alias('ID'))              \n",
    "\n",
    "# Create Inverted Index\n",
    "indice = idx.groupBy(idx['Celda']).agg(collect_list(idx['ID']).alias(\"Listas\"))\n",
    "indice = indice.select(indice['Celda'].cast(IntegerType()),indice['Listas'])\n",
    "\n",
    "# (Optional )Repartition for avoid uneven partitions\n",
    "#indice = indice.repartition(10000,indice['Celda']).sortWithinPartitions(indice['Celda'])\n",
    "\n",
    "# Save Dataframe in Cache (RAM)\n",
    "indice.cache()\n",
    "\n",
    "# Action to run execution graph\n",
    "indice.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (Optional) Save and Load index - Hard Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Save index in an ORC File\n",
    "indice.write.orc(\"Path to Write File\")\n",
    "\n",
    "# Load index from ORC File\n",
    "indice = sc.read.orc(\"Path from ORC File\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## UDF's for Dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#UDF's - Calculating the mode from a vector and calculating the mode from a vector of vectors\n",
    "def moda_vector(x):\n",
    "    flattened = [val for sublist in x for val in sublist]\n",
    "    return mode(flattened)\n",
    "    \n",
    "moda = udf(lambda x:mode(x))\n",
    "moda_2 = udf(lambda x:moda_vector(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading PointClouds Queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Loading PointClouds Queries from CSV File\n",
    "dfq = sc.read.format('csv').options(header='true', inferschema='true').csv('path from CSV File')\n",
    "\n",
    "# For every point calculate corresponding cell\n",
    "datosq = dfq.select(dfq['I'].cast(IntegerType()), floor(dfq['X']/delta).alias(\"X\"), floor(dfq['Y']/delta).alias(\"Y\"))\n",
    "dq = datosq.select((datosq['X']+ (cmax/delta)*datosq['Y']).cast(IntegerType()).alias(\"Celda\"),datosq['I'].cast(IntegerType()).alias(\"ID\")) \n",
    "\n",
    "#  (Optional )Repartition for avoid uneven partitions\n",
    "#dq = dq.repartition(10000,dq['Celda']).sortWithinPartitions(dq['Celda'])\n",
    "\n",
    "# Save Dataframe in Cache (RAM)\n",
    "dq.cache()\n",
    "\n",
    "# Action to run execution graph\n",
    "dq.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Querying Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Join Query - Qyerying all PointClouds in Batch\n",
    "answer = indice.join(dq, \"Celda\")\n",
    "answer = answer.select(answer[\"ID\"].alias(\"QueryID\"),answer[\"Listas\"].alias(\"ListasCloudID\"))\n",
    "anw = answer.groupBy(answer['QueryID']).agg(collect_list(answer['ListasCloudID']).alias(\"ListasID\"))\n",
    "\n",
    "# 1-NN\n",
    "anw = anw.select(anw['QueryID'],moda_2(anw['ListasID']).alias(\"NubeID\"))\n",
    "\n",
    "# Recall@1\n",
    "recall = anw.select(avg((anw['QueryID']==anw['NubeID']).cast(IntegerType())).alias(\"Recall@1\"))\n",
    "\n",
    "# Show Recall@1\n",
    "recall.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Stop Spark Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
